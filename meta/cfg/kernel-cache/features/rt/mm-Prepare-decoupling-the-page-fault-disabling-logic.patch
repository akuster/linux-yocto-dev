From acceb0761d8b60c5761e7d6b820f1ed732419b34 Mon Sep 17 00:00:00 2001
From: Ingo Molnar <mingo@elte.hu>
Date: Fri, 3 Jul 2009 08:30:37 -0500
Subject: [PATCH 029/309] mm: Prepare decoupling the page fault disabling logic

Add a pagefault_disabled variable to task_struct to allow decoupling
the pagefault-disabled logic from the preempt count.

Signed-off-by: Ingo Molnar <mingo@elte.hu>
Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
---
 include/linux/sched.h   |  1 +
 include/linux/uaccess.h | 32 +++-----------------------------
 kernel/fork.c           |  1 +
 mm/memory.c             | 30 ++++++++++++++++++++++++++++++
 4 files changed, 35 insertions(+), 29 deletions(-)

diff --git a/include/linux/sched.h b/include/linux/sched.h
index fdfdfb8e92b5..2e47dc051549 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1388,6 +1388,7 @@ struct task_struct {
 	/* mutex deadlock detection */
 	struct mutex_waiter *blocked_on;
 #endif
+	int pagefault_disabled;
 #ifdef CONFIG_TRACE_IRQFLAGS
 	unsigned int irq_events;
 	unsigned long hardirq_enable_ip;
diff --git a/include/linux/uaccess.h b/include/linux/uaccess.h
index ecd3319dac33..9414a1b48f5c 100644
--- a/include/linux/uaccess.h
+++ b/include/linux/uaccess.h
@@ -6,36 +6,10 @@
 
 /*
  * These routines enable/disable the pagefault handler in that
- * it will not take any locks and go straight to the fixup table.
- *
- * They have great resemblance to the preempt_disable/enable calls
- * and in fact they are identical; this is because currently there is
- * no other way to make the pagefault handlers do this. So we do
- * disable preemption but we don't necessarily care about that.
+ * it will not take any MM locks and go straight to the fixup table.
  */
-static inline void pagefault_disable(void)
-{
-	preempt_count_inc();
-	/*
-	 * make sure to have issued the store before a pagefault
-	 * can hit.
-	 */
-	barrier();
-}
-
-static inline void pagefault_enable(void)
-{
-#ifndef CONFIG_PREEMPT
-	/*
-	 * make sure to issue those last loads/stores before enabling
-	 * the pagefault handler again.
-	 */
-	barrier();
-	preempt_count_dec();
-#else
-	preempt_enable();
-#endif
-}
+extern void pagefault_disable(void);
+extern void pagefault_enable(void);
 
 #ifndef ARCH_HAS_NOCACHE_UACCESS
 
diff --git a/kernel/fork.c b/kernel/fork.c
index fdf3910dd165..2faa86f49773 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1298,6 +1298,7 @@ static struct task_struct *copy_process(unsigned long clone_flags,
 	p->hardirq_context = 0;
 	p->softirq_context = 0;
 #endif
+	p->pagefault_disabled = 0;
 #ifdef CONFIG_LOCKDEP
 	p->lockdep_depth = 0; /* no locks held yet */
 	p->curr_chain_key = 0;
diff --git a/mm/memory.c b/mm/memory.c
index 81813d99af73..e89ae186b5b6 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -3690,6 +3690,36 @@ unlock:
 	return 0;
 }
 
+void pagefault_disable(void)
+{
+	preempt_count_inc();
+	current->pagefault_disabled++;
+	/*
+	 * make sure to have issued the store before a pagefault
+	 * can hit.
+	 */
+	barrier();
+}
+EXPORT_SYMBOL(pagefault_disable);
+
+void pagefault_enable(void)
+{
+#ifndef CONFIG_PREEMPT
+	/*
+	 * make sure to issue those last loads/stores before enabling
+	 * the pagefault handler again.
+	 */
+	barrier();
+	current->pagefault_disabled--;
+	preempt_count_dec();
+#else
+	barrier();
+	current->pagefault_disabled--;
+	preempt_enable();
+#endif
+}
+EXPORT_SYMBOL(pagefault_enable);
+
 /*
  * By the time we get here, we already hold the mm semaphore
  */
-- 
1.8.1.2

